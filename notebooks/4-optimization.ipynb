{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Optimization  \n",
    "\n",
    "Now that we have a structured evaluation process, it’s time to **optimize and refine** our system.  \n",
    "\n",
    "In this part, we’ll:  \n",
    "- **Test different models** to compare their strengths.  \n",
    "- **Analyze trade-offs** between speed, performance, and cost.  \n",
    "- **Make informed decisions** about improvements and potential deployment.  \n",
    "\n",
    "This is where we shift from a prototype to something more **robust and practical**. Whether you're thinking about deploying an LLM system or simply improving local performance, this step will help you make data-driven optimizations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there's no free lunch. \n",
    "Let's change some inference parameters and see if we can get a speedup,\n",
    "but use our evals to understand what the tradeoff is.\n",
    "\n",
    "## Steps\n",
    "* Run inference but this time focus on engineering metrics, in our case we care about clock time\n",
    "* Measure 5 responses using the an fp16 model, then repeat with a fp2 model\n",
    "* Before running evals, what is different about these models?\n",
    "  * Hints: Google quantization, look at the model size on Ollama, think about what is expensive during inference.\n",
    "* After runing evals\n",
    "  * What do you notice is different?\n",
    "  * What about the quality of the responses? How does that change?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\n\\n**A Journey Through Time: The History and Achievements of North Melbourne Kangaroos**\\n\\nThe North Melbourne Kangaroos, affectionately known as the Magpies prior to their 1983 rebranding, are a storied team with a rich tapestry of history. Nestled in Ballarat, Victoria, these kangaroos have been the pride of the region since their inception in 1924. This article delves into their remarkable journey, from their early days as a minor league side to becoming a significant force in Australian rugby union.\\n\\n**The Founding of the Kangaroos**\\n\\nNorth Melbourne Kangaroos were born on November 6, 1924, when the North Melbourne Rugby Union Club was established. The club\\'s name change to Kangaroos occurred after their 1983 merger with Torquay United, a move that reflected their transition from being based in Melbourne to Ballarat.\\n\\n**A Legacy of Excellence**\\n\\nThe Kangaroos\\' history is marked by excellence and resilience. Over the decades, they\\'ve claimed multiple State Titles, showcasing their dominance on the field. Notable players such as David Nasmith and Adam Droms have been luminaries of the team, with Nasmith holding the distinction of being one of the youngest players to grace the field in 1958 and still active today.\\n\\n**Current Stance in the League**\\n\\nSince their move to Ballarat in 2019, the North Melbourne Kangaroos have become a prominent side in the Victorian top-tier competition. Their transition was met with both anticipation and nostalgia from their old fans, who cherished the memories of playing at Belmore Ground, the club\\'s historic home.\\n\\n**Community Connection**\\n\\nThe kangaroos are deeply rooted in the Ballarat community. Their branding includes the iconic \"Kangaroo\" logo, symbolizing their connection to the local wildlife. The team has also embraced community involvement, with players and staff engaging actively in local initiatives, fostering a strong bond between the club and its surroundings.\\n\\n**Engaging with Fans**\\n\\nIn keeping with rugby union traditions, the Kangaroos embrace close-knit relationships with their fans. Whether through match days or annual events, the team consistently demonstrates a commitment to community spirit, further cementing its identity as a local institution.\\n\\n**Looking Ahead**\\n\\nThe future of North Melbourne Kangaroos looks promising. With a focus on developing young talent and maintaining high standards, the club is poised for continued success. Fans can expect an exciting season ahead, with the kangaroos aiming to uphold their legacy while exploring new horizons in the league.\\n\\nIn conclusion, the North Melbourne Kangaroos represent more than just a rugby team; they are a symbol of pride for Ballarat and a testament to the enduring spirit of sportsmanship. As they continue to navigate the challenges and opportunities of modern rugby union, one can look forward to many thrilling matches and memorable moments in their storied history.'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is about an Australian team.\\n\\nThe passage mentions the North Melbourne Kangaroos, an Australian rugby union team, and refers to them as such throughout the article.',\n",
       " 2.1349551677703857)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'gemma:2b'\n",
    "\n",
    "def single_turn_with_time(prompt, model):\n",
    "    start_time = time.time()\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    return response.message.content, total_time\n",
    "\n",
    "\n",
    "prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "single_turn_with_time(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"gemma2:2b-text-fp16\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m      7\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Is this about an australian or american team?, print no other words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msingle_turn_with_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msingle_turn_with_time\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingle_turn_with_time\u001b[39m(prompt, model):\n\u001b[32m      4\u001b[39m     start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     response: ChatResponse = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     end_time = time.time()\n\u001b[32m     13\u001b[39m     total_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ai_app_basics/gemma-app/lib/python3.11/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ai_app_basics/gemma-app/lib/python3.11/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ai_app_basics/gemma-app/lib/python3.11/site-packages/ollama/_client.py:122\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model \"gemma2:2b-text-fp16\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "# os.environ[\"OLLAMA_KV_CACHE_TYPE\"] = \"FP16\"\n",
    "# os.environ[\"OLLAMA_FLASH_ATTENTION\"] = \"0\"\n",
    "\n",
    "model = 'gemma2:2b-text-fp16'\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = f\"{i} Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    print(single_turn_with_time(prompt, model=model)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    prompt = f\"{i} Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    response, wall_time = single_turn_with_time(prompt, model=model)\n",
    "    character_per_second = len(response) / wall_time\n",
    "    print(character_per_second, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized model\n",
    "Let's try the quantized model. See if there's anything you notice\n",
    "\n",
    "### Key Points for students\n",
    "* Notice that the tokens per second is much faster\n",
    "* Also notice though that the quality is much worse. The model is outputting all sorts of weird characters and is barely usable\n",
    "* Is this tradeoff worth it? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.68002200126648\n",
      "1.0478029251098633\n",
      "1.0427780151367188\n",
      "1.0453429222106934\n",
      "1.0419139862060547\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"OLLAMA_KV_CACHE_TYPE\"] = \"q4_0\"\n",
    "\n",
    "model = 'gemma2:2b-instruct-q2_K'\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = f\"{i} Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    print(single_turn_with_time(prompt, model=model)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.316331449822416 Australian \n",
      "\n",
      "103.2346189144231 American \n",
      "\n",
      "\n",
      "**Note:** I was unable to find enough evidence to be certain that it is not related to Australia.  However, this article is about the North Melbourne Kangaroos which is an Australian team. \n",
      "\n",
      "11.496136255246524 Australian \n",
      "\n",
      "9.590387076918292 American \n",
      "\n",
      "9.60936875330341 American \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemma2:2b-instruct-q2_K'\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = f\"{i} Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    response, wall_time = single_turn_with_time(prompt, model=model)\n",
    "    character_per_second = len(response) / wall_time\n",
    "    print(character_per_second, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
