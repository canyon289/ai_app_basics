{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Building the MVP AI System  \n",
    "\n",
    "In this first part, we’ll build a **zero-shot classification system** using a local **LLM-powered approach**. Instead of training a model from scratch, we’ll leverage **Gemma 2B**’s in-context learning to classify sports teams as **from the US or Australia**—without any fine-tuning.  \n",
    "\n",
    "But an AI system is **more than just a model**. We'll also integrate:  \n",
    "- **Gradio** to build an interactive front end.  \n",
    "- **SQLite** to store data and results.  \n",
    "- **Datasette** for observability, allowing us to inspect predictions and iterate effectively.  \n",
    "\n",
    "By the end of this section, you’ll have a **working MVP AI system**—a functional app with a front end, database, and structured observability to track and refine performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gemma 2B and Ollama?\n",
    "\n",
    "### 🔹 Gemma 2B: A Small but Mighty LLM  \n",
    "\n",
    "![Alt text](img/gemma2-2B.png)\n",
    "\n",
    "[Gemma](https://ai.google.dev/gemma) is a family of open-weight models from Google DeepMind, designed for efficiency and strong reasoning capabilities.  \n",
    "- **2B parameters**: Small enough to run locally but powerful enough for real tasks.  \n",
    "- **Supports in-context learning**: Like other transformer models, Gemma can classify and generate outputs based on provided examples without additional training.  \n",
    "- **Works well for zero-shot classification**: Like other modern LLMs, Gemma can classify inputs based on prompts without fine-tuning.  \n",
    "- **Fast and cost-effective**: Unlike larger models, Gemma 2B runs efficiently on consumer hardware.  \n",
    "\n",
    "### 🚀 Ollama: A Game Changer for Local LLMs  \n",
    "\n",
    "![Alt text](img/ollama.svg)\n",
    "\n",
    "[Ollama](https://ollama.com/) makes running **LLMs locally** seamless, without complex setup.  \n",
    "- **Pre-configured model execution**: No need to manually set up dependencies.  \n",
    "- **Efficient GPU/CPU inference**: Optimized for running on local machines.  \n",
    "- **Fast iteration loop**: Load a model once, then run queries without excessive overhead.  \n",
    "\n",
    "By combining **Gemma 2B** with **Ollama**, we get a **lightweight, fast, and cost-free AI system** that can perform real-world classification tasks directly on our machines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone! 👋  😊 \\n\\nIs there anything I can help you with today? \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model = 'gemma2:2b'\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "prompt = \"Say hello to the class\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our zero-shot classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_team = \"Carlton Blues\"\n",
    "american_team = \"Tennessee Titans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australian \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Output if this is an australian or american team, only print australian or american no other output: \" + f\"{afl_team}\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'American \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Output if this is an australian or american team, only print australian or american no other output: \" + f\"{american_team}\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Creating Our Gradio App  \n",
    "\n",
    "![Alt text](img/gradio.png)\n",
    "\n",
    "Before we dive into the code, let's talk about **Gradio**—one of the easiest ways to spin up interactive front-ends for AI applications.  \n",
    "\n",
    "🚀 **Why Gradio?**  \n",
    "- **Super fast MVP development**: Build an interactive AI demo in just a few lines of code.  \n",
    "- **No frontend experience required**: Just define a Python function, and Gradio handles the UI.  \n",
    "- **Part of the 🤗 Hugging Face ecosystem**: Seamlessly integrates with **models, Spaces, and APIs**.  \n",
    "- **Great for rapid prototyping**: Test AI models with real users before scaling up.  \n",
    "\n",
    "We'll use **Gradio** to build an interactive app that lets users test **Gemma 2B** for zero-shot classification—without needing a separate web framework. Let's get started! 🚀  \n",
    "\n",
    "\n",
    "For instruction purposes, we've included the code below, but we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "model = 'gemma:2b'\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    response = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 What's Happening in This Code?  \n",
    "\n",
    "- 🔄 **Imports Gradio & Ollama** – We bring in the tools we need to build the UI and interact with the model.  \n",
    "- 🧠 **Defines the model** – We're using **Gemma 2B** (`gemma:2b`) to power the chatbot.  \n",
    "- 💬 **Creates a function (`chat_with_model`)** – Sends user input to the model via **Ollama** and returns a response.  \n",
    "- 🎨 **Builds the Gradio UI (`iface`)** –  \n",
    "  - **📩 Input**: A text box for user messages.  \n",
    "  - **🖥️ Output**: The model's response.  \n",
    "  - **🎭 Title & Description**: A simple interface for chatting with Gemma.  \n",
    "- 🚀 **Launches the app!** – Runs the interactive chatbot in your browser.  \n",
    "\n",
    "Now, let’s fire it up and start chatting! 🔥  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability with SQLite and Datasette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Why Tracing & Observability Matter\n",
    "\n",
    "Building an AI system isn’t just about **getting a response**—it’s about **understanding and improving how your model behaves over time**.  \n",
    "- **👀 Observability** helps us track inputs, outputs, and model decisions, making debugging and iteration easier.  \n",
    "- **🐛 Tracing conversations** lets us spot patterns, catch failure cases, and fine-tune our system for better performance.  \n",
    "- **📈 Data-Driven Decisions**: Instead of guessing if the model is working well, we can use **real logged interactions** to refine prompts, improve accuracy, and compare models.  \n",
    "\n",
    "## 🗄️ Why SQLite? A No-Brainer for MVPs  \n",
    "\n",
    "![Alt text](img/sqlite.png)\n",
    "\n",
    "For **early-stage apps**, **SQLite** is an **ideal** choice for logging and observability:  \n",
    "- **🛠️ No Setup Hassle** – It’s a self-contained, file-based database. No server required.  \n",
    "- **⚡ Fast & Lightweight** – Handles reads and writes efficiently without extra overhead.  \n",
    "- **📦 Portable & Easy to Share** – Just a single file (`.db`) that works across different environments.  \n",
    "- **🔗 Overwhelmingly Popular** – Used in everything from **mobile apps (iOS, Android)** to **browsers (Chrome, Firefox)** and even **airplane black boxes**!  \n",
    "\n",
    "### 🚀 Future Scaling  \n",
    "Right now, **SQLite is perfect** for logging and inspecting model interactions. Later, if we move to **multi-user or production-scale apps**, we can switch to **PostgreSQL, MySQL, or cloud-based solutions**—but for now, SQLite keeps things simple and effective.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, we’ll **log our prompts and responses** so we can start analyzing how our system is performing! 🔍\n",
    "\n",
    " As above, we've included the code below, although we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# SQLite Database Setup\n",
    "DB_PATH = \"chat_log.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create a simple SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prompt TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()  # Ensure the DB is set up before running the app\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    \"\"\"Send user input to Ollama, get response, and log to SQLite.\"\"\"\n",
    "    response = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])[\"message\"][\"content\"]\n",
    "    \n",
    "    # Log the interaction to SQLite\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO chat_history (prompt, response) VALUES (?, ?)\", (prompt, response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model. Your chats are logged in SQLite.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 What's Happening in This Code?  \n",
    "\n",
    "- 📦 **Imports required libraries** –  \n",
    "  - `gradio` for the UI  \n",
    "  - `ollama` for running **Gemma 2B**  \n",
    "  - `sqlite3` for logging interactions  \n",
    "  - `datetime` to track timestamps  \n",
    "\n",
    "- 🗄️ **Sets up a SQLite database (`chat_log.db`)** –  \n",
    "  - Creates a **`chat_history`** table (if it doesn’t exist)  \n",
    "  - Stores **`prompt`**, **`response`**, and **timestamp** for each chat  \n",
    "\n",
    "- 💬 **Defines `chat_with_model(prompt)`** –  \n",
    "  - Sends user input to **Ollama (Gemma 2B)**  \n",
    "  - Logs the chat to **SQLite**  \n",
    "  - Returns the model’s response  \n",
    "\n",
    "- 🎨 **Creates a Gradio UI (`iface`)** –  \n",
    "  - **📝 Input:** A text box for user queries  \n",
    "  - **🖥️ Output:** The model’s response  \n",
    "  - **📜 Description:** Informs users that chats are logged  \n",
    "\n",
    "- 🚀 **Launches the app!** – Runs a browser-based chatbot with full logging  \n",
    "\n",
    "This setup ensures we can **track every interaction**, making debugging, evaluation, and iteration much easier. Next, let's test it out! 🔍  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Exploring Your Data with Datasette  \n",
    "\n",
    "![Alt text](img/datasette.png)\n",
    "\n",
    "Once we’ve logged conversations in **SQLite**, we need an easy way to inspect and analyze them.  \n",
    "That’s where **[Datasette](https://datasette.io/)** comes in—a powerful tool for **browsing, querying, and exporting SQLite databases** effortlessly.  \n",
    "\n",
    "### 🚀 Why Datasette?  \n",
    "- **Instant Database UI** – No SQL knowledge required; just open a browser and explore!  \n",
    "- **Lightning Fast** – Designed for large-scale data publishing but perfect for small logs too.  \n",
    "- **Built-in Querying** – Filter, sort, and search directly in a web-based UI.  \n",
    "- **Easy Exporting** – Convert your database into **CSV**, **JSON**, or other formats with a click.  \n",
    "\n",
    "### 📤 Exporting Traces to CSV  \n",
    "\n",
    "We’ll use **Datasette** to **export chat logs to a CSV file**, making it easier to analyze failure cases and refine our AI system.  \n",
    "This CSV can be used for:  \n",
    "- **📊 Failure Mode Analysis** – Identify common mistakes by reviewing responses.  \n",
    "- **👥 Sharing with Subject Matter Experts** – Non-technical teammates can review and give feedback.  \n",
    "- **✅ Manual Evaluation** – Open in a spreadsheet and score outputs with 👍/👎 + comments.  \n",
    "- **📈 Starting Systematic Evaluations** – Lay the groundwork for automated performance tracking.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, let’s load up **Datasette**, explore our logged chats, and **export them for further analysis!** 🧐📊  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Recap: What We Learned  \n",
    "\n",
    "In this notebook, we focused on building the **first version of an LLM-powered classification system** that runs **entirely locally**. Here’s what we covered:  \n",
    "\n",
    "### 🏗️ **Building an MVP AI System**  \n",
    "- Used **Gemma 2B** + **Ollama** to create a **zero-shot classification model**.  \n",
    "- Built an interactive **Gradio** UI to test our system.  \n",
    "\n",
    "### 🔍 **Logging & Observability**  \n",
    "- Stored model interactions in an **SQLite** database for **tracing and debugging**.  \n",
    "- Used **Datasette** to **browse logged conversations and export data**.  \n",
    "\n",
    "### 📤 **Exporting for Further Exploration**  \n",
    "- Learned how to **export chat logs to CSV** for potential later analysis.  \n",
    "- Discussed how **structured logs** help track model responses over time.  \n",
    "\n",
    "### 🚀 **Why This Matters**  \n",
    "- **AI systems are more than just models—they need observability and traceability.**  \n",
    "- **Logging interactions** makes debugging, iteration, and improvement possible.  \n",
    "- This lays the **foundation** for deeper **evaluation techniques** in upcoming sections.  \n",
    "\n",
    "In the next notebook, we’ll take things further by **evaluating our model using vibes**—testing different prompts and seeing how well they work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
