{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Building the MVP AI System  \n",
    "\n",
    "In this first part, we’ll build a **zero-shot classification system** using a local **LLM-powered approach**. Instead of training a model from scratch, we’ll leverage **Gemma 2B**’s in-context learning to classify sports teams as **from the US or Australia**—without any fine-tuning.  \n",
    "\n",
    "But an AI system is **more than just a model**. We'll also integrate:  \n",
    "- **Gradio** to build an interactive front end.  \n",
    "- **SQLite** to store data and results.  \n",
    "- **Datasette** for observability, allowing us to inspect predictions and iterate effectively.  \n",
    "\n",
    "By the end of this section, you’ll have a **working MVP AI system**—a functional app with a front end, database, and structured observability to track and refine performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's great to be here with you all. How can I help you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model = 'gemma:2b'\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "prompt = \"Say hello to the class\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our zero-shot classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_team = \"Carlton Blues\"\n",
    "american_team = \"Tennessee Titans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text is about an Australian team, so it is an Australian team and the output is \"Australian\".'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Output if this is an australian or american team, only print australian or american no other output: \" + f\"{afl_team}\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The team's origin is not specified in the context, so I cannot determine if it is an Australian or American team.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Output if this is an australian or american team, only print australian or american no other output: \" + f\"{american_team}\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create our Gradio app. For instruction purposes, we've included the code below, but we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "model = 'gemma:2b'\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    response = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability with SQLite and Datasette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now log our traces (prompt + response) to a local (SQLite) database, in order to have consistent visbility into our system. As above, we've included the code below, although we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# SQLite Database Setup\n",
    "DB_PATH = \"chat_log.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create a simple SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prompt TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()  # Ensure the DB is set up before running the app\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    \"\"\"Send user input to Ollama, get response, and log to SQLite.\"\"\"\n",
    "    response = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])[\"message\"][\"content\"]\n",
    "    \n",
    "    # Log the interaction to SQLite\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO chat_history (prompt, response) VALUES (?, ?)\", (prompt, response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model. Your chats are logged in SQLite.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of ways to look at your data in your SQLite database. We'll be using Simon Willison's `Datasette` tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
